{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Autonomous Insurance Claims Processing Agent\n",
        "============================================\n",
        "\n",
        "This script implements a comprehensive document processing pipeline\n",
        "for insurance FNOL (First Notice of Loss) forms.\n",
        "\n",
        "Pipeline Responsibilities:\n",
        "1. Read PDF document\n",
        "2. Extract raw text content\n",
        "3. Parse structured fields (18 fields total)\n",
        "4. Validate mandatory information\n",
        "5. Apply business routing rules\n",
        "6. Produce structured JSON output\n",
        "\n",
        "\n",
        "This implementation uses deterministic pattern matching for reliability\n",
        "and reproducibility without requiring external paid APIs."
      ],
      "metadata": {
        "id": "JpEqstUH-8bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 1: DEPENDENCY INSTALLATION\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Install Required Python Packages\n",
        "---------------------------------\n",
        "\n",
        "This section installs all necessary dependencies for document processing,\n",
        "validation, and data handling.\n",
        "\n",
        "Packages:\n",
        "- pymupdf: Fast PDF text extraction\n",
        "- pydantic: Data validation and schema enforcement\n",
        "- python-dotenv: Environment variable management\n",
        "\"\"\"\n",
        "\n",
        "!pip install pymupdf pydantic python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx351umU--Zj",
        "outputId": "2d1a4868-d70c-461c-bac2-78e351738201"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.7)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: IMPORT LIBRARIES\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "import json\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "from typing import Dict, List, Optional, Any\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "BcbDmWAb_B0w"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 3: PDF TEXT EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "def extract_text_from_pdf(pdf_file) -> str:\n",
        "    \"\"\"\n",
        "    Extract all text content from a PDF file.\n",
        "\n",
        "    This function opens a PDF document and iterates through all pages,\n",
        "    extracting readable text content from each page. The text is\n",
        "    concatenated into a single string for downstream processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pdf_file : file object\n",
        "        Binary file object of the PDF document (opened in 'rb' mode)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Complete text content extracted from all pages of the PDF\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Uses PyMuPDF (fitz) library for fast and reliable parsing\n",
        "    - Handles multi-page documents automatically\n",
        "    - Preserves text layout and formatting where possible\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> with open(\"claim.pdf\", \"rb\") as f:\n",
        "    ...     text = extract_text_from_pdf(f)\n",
        "    >>> print(text[:100])\n",
        "    \"\"\"\n",
        "\n",
        "    # Open PDF from binary stream\n",
        "    # filetype=\"pdf\" explicitly specifies the document format\n",
        "    doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
        "\n",
        "    # Initialize empty string to accumulate text\n",
        "    text = \"\"\n",
        "\n",
        "    # Iterate through all pages in the document\n",
        "    for page_num, page in enumerate(doc, start=1):\n",
        "        # Extract text from current page and append to result\n",
        "        page_text = page.get_text()\n",
        "        text += page_text\n",
        "\n",
        "        # Optional: Add page separator for debugging\n",
        "        # text += f\"\\n--- End of Page {page_num} ---\\n\"\n",
        "\n",
        "    # Close the document to free resources\n",
        "    doc.close()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "H18pYwmS_End"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: FIELD EXTRACTION ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "def extract_claim_fields(text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract structured claim information from raw PDF text.\n",
        "\n",
        "    This function uses regular expressions to locate and extract\n",
        "    18 distinct fields from insurance claim documents. Fields are\n",
        "    organized into categories: Policy Info, Incident Info, Parties,\n",
        "    Asset Details, and Other Mandatory Fields.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        Raw text content extracted from the PDF document\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary containing all extracted claim fields organized by category\n",
        "\n",
        "        Structure:\n",
        "        {\n",
        "            \"policy_number\": str,\n",
        "            \"policyholder_name\": str,\n",
        "            \"effective_date_start\": str,\n",
        "            \"effective_date_end\": str,\n",
        "            \"incident_date\": str,\n",
        "            \"incident_time\": str,\n",
        "            \"location\": str,\n",
        "            \"description\": str,\n",
        "            \"claimant_name\": str,\n",
        "            \"claimant_contact\": str,\n",
        "            \"third_party_name\": str,\n",
        "            \"third_party_contact\": str,\n",
        "            \"asset_type\": str,\n",
        "            \"asset_id\": str,\n",
        "            \"estimated_damage\": int,\n",
        "            \"claim_type\": str,\n",
        "            \"attachments\": str,\n",
        "            \"initial_estimate\": int\n",
        "        }\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Uses case-insensitive regex matching (re.I flag)\n",
        "    - Numeric fields are converted to integers\n",
        "    - Missing fields will have None values\n",
        "    - Handles variations in field label formatting\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> text = \"Policy Number: POL-12345\\\\nDate of Loss: 02/15/2024\"\n",
        "    >>> fields = extract_claim_fields(text)\n",
        "    >>> print(fields[\"policy_number\"])\n",
        "    POL-12345\n",
        "    \"\"\"\n",
        "\n",
        "    def find_field(pattern: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Helper function to search for a specific field pattern.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pattern : str\n",
        "            Regular expression pattern to search for\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str or None\n",
        "            Extracted value if found, None otherwise\n",
        "        \"\"\"\n",
        "        match = re.search(pattern, text, re.I)  # re.I = case-insensitive\n",
        "        if match:\n",
        "            value = match.group(1).strip()\n",
        "            # Return None for empty values or values that are just labels/headers\n",
        "            # OR is a known section header (all caps and contains \"DETAILS\" or similar)\n",
        "            if not value:\n",
        "                return None\n",
        "            if value.endswith(':'):\n",
        "                return None\n",
        "            if value in ['ASSET DETAILS', 'OTHER INFORMATION', 'INVOLVED PARTIES', 'INCIDENT INFORMATION', 'POLICY INFORMATION']:\n",
        "                return None\n",
        "            return value\n",
        "        return None\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # POLICY INFORMATION FIELDS\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    policy_number = find_field(r\"Policy Number:\\s*(.+?)(?:\\n|$)\")\n",
        "    policyholder_name = find_field(r\"Policyholder Name:\\s*(.+?)(?:\\n|$)\")\n",
        "    effective_date_start = find_field(r\"Effective Date \\(Start\\):\\s*(.+?)(?:\\n|$)\")\n",
        "    effective_date_end = find_field(r\"Effective Date \\(End\\):\\s*(.+?)(?:\\n|$)\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # INCIDENT INFORMATION FIELDS\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    incident_date = find_field(r\"Date of Loss:\\s*(.+?)(?:\\n|$)\")\n",
        "    incident_time = find_field(r\"Time of Loss:\\s*([^\\n:]+?)(?=\\n[A-Z]|$)\")\n",
        "    location = find_field(r\"Location:\\s*(.+?)(?:\\n|$)\")\n",
        "\n",
        "    # Description may span multiple lines, so use a more flexible pattern\n",
        "    # Use DOTALL flag to match across newlines\n",
        "    description_match = re.search(\n",
        "        r\"Description of Accident:\\s*(.+?)(?=\\nINVOLVED PARTIES|\\nClaimant Name:|\\nASSET DETAILS|\\nOTHER INFORMATION|$)\",\n",
        "        text,\n",
        "        re.I | re.DOTALL  # This flag makes it capture multi-line text\n",
        "    )\n",
        "    # IMPORTANT: Check description_match first, THEN assign description\n",
        "    if description_match:\n",
        "        # Clean up extra whitespace and newlines\n",
        "        description = \" \".join(description_match.group(1).strip().split())\n",
        "    else:\n",
        "        description = None\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # INVOLVED PARTIES FIELDS\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    claimant_name = find_field(r\"Claimant Name:\\s*(.+?)(?:\\n|$)\")\n",
        "    claimant_contact = find_field(r\"Claimant Contact:\\s*(.+?)(?:\\n|$)\")\n",
        "    third_party_name = find_field(r\"Third Party Name:\\s*(.+?)(?:\\n|$)\")\n",
        "    third_party_contact = find_field(r\"Third Party Contact:\\s*(.+?)(?:\\n|$)\")\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # ASSET DETAILS FIELDS\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    asset_type = find_field(r\"Asset Type:\\s*(.+?)(?:\\n|$)\")\n",
        "    asset_id = find_field(r\"Asset ID:\\s*(.+?)(?:\\n|$)\")\n",
        "\n",
        "    # Extract numeric damage amount\n",
        "    estimated_damage_str = find_field(r\"Estimated Damage:\\s*\\$?(\\d+)\")\n",
        "    estimated_damage = int(estimated_damage_str) if estimated_damage_str else None\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # OTHER MANDATORY FIELDS\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    claim_type = find_field(r\"Claim Type:\\s*(.+?)(?:\\n|$)\")\n",
        "    attachments = find_field(r\"Attachments:\\s*([^\\n:]+?)(?=\\n[A-Z]|$)\")\n",
        "\n",
        "    # Extract numeric initial estimate\n",
        "    initial_estimate_str = find_field(r\"Initial Estimate:\\s*\\$?(\\d+)\")\n",
        "    initial_estimate = int(initial_estimate_str) if initial_estimate_str else None\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # ASSEMBLE ALL FIELDS INTO DICTIONARY\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    extracted_data = {\n",
        "        # Policy Information\n",
        "        \"policy_number\": policy_number,\n",
        "        \"policyholder_name\": policyholder_name,\n",
        "        \"effective_date_start\": effective_date_start,\n",
        "        \"effective_date_end\": effective_date_end,\n",
        "\n",
        "        # Incident Information\n",
        "        \"incident_date\": incident_date,\n",
        "        \"incident_time\": incident_time,\n",
        "        \"location\": location,\n",
        "        \"description\": description,\n",
        "\n",
        "        # Involved Parties\n",
        "        \"claimant_name\": claimant_name,\n",
        "        \"claimant_contact\": claimant_contact,\n",
        "        \"third_party_name\": third_party_name,\n",
        "        \"third_party_contact\": third_party_contact,\n",
        "\n",
        "        # Asset Details\n",
        "        \"asset_type\": asset_type,\n",
        "        \"asset_id\": asset_id,\n",
        "        \"estimated_damage\": estimated_damage,\n",
        "\n",
        "        # Other Mandatory Fields\n",
        "        \"claim_type\": claim_type,\n",
        "        \"attachments\": attachments,\n",
        "        \"initial_estimate\": initial_estimate\n",
        "    }\n",
        "\n",
        "    return extracted_data"
      ],
      "metadata": {
        "id": "GHFh_xVM_Ixg"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 5: VALIDATION ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "# Define the list of fields that MUST be present for automated processing\n",
        "MANDATORY_FIELDS = [\n",
        "    \"policy_number\",\n",
        "    \"policyholder_name\",\n",
        "    \"incident_date\",\n",
        "    \"location\",\n",
        "    \"description\",\n",
        "    \"claimant_name\",\n",
        "    \"asset_type\",\n",
        "    \"claim_type\",\n",
        "    \"estimated_damage\",\n",
        "    \"initial_estimate\"\n",
        "]\n",
        "\n",
        "\n",
        "def identify_missing_fields(extracted_data: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identify which mandatory fields are missing or empty.\n",
        "\n",
        "    This function checks all required fields to ensure they have\n",
        "    valid values. Empty strings, None values, and zero numeric\n",
        "    values are considered missing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    extracted_data : dict\n",
        "        Dictionary containing extracted claim fields\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of str\n",
        "        List of field names that are missing or invalid\n",
        "        Empty list if all mandatory fields are present\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Checks for None, empty strings, and whitespace-only strings\n",
        "    - Numeric fields (damage amounts) must be > 0\n",
        "    - Case-sensitive field name matching\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data = {\"policy_number\": \"POL-123\", \"policyholder_name\": \"\"}\n",
        "    >>> missing = identify_missing_fields(data)\n",
        "    >>> print(missing)\n",
        "    ['policyholder_name', 'incident_date', ...]\n",
        "    \"\"\"\n",
        "\n",
        "    missing_fields = []\n",
        "\n",
        "    for field_name in MANDATORY_FIELDS:\n",
        "        field_value = extracted_data.get(field_name)\n",
        "\n",
        "        # Check if field is missing, None, empty string, or whitespace-only\n",
        "        if field_value is None:\n",
        "            missing_fields.append(field_name)\n",
        "        elif isinstance(field_value, str) and not field_value.strip():\n",
        "            missing_fields.append(field_name)\n",
        "        elif isinstance(field_value, (int, float)) and field_value <= 0:\n",
        "            # Numeric fields like estimated_damage must be positive\n",
        "            missing_fields.append(field_name)\n",
        "\n",
        "    return missing_fields\n"
      ],
      "metadata": {
        "id": "B6uMpsDd_Qp6"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 6: ROUTING ENGINE\n",
        "# ============================================================================\n",
        "\n",
        "def route_claim(extracted_data: Dict[str, Any],\n",
        "                missing_fields: List[str]) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Determine the appropriate processing route for a claim.\n",
        "\n",
        "    This function implements business logic rules to classify claims\n",
        "    and route them to the correct workflow queue. Rules are evaluated\n",
        "    in priority order.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    extracted_data : dict\n",
        "        Dictionary containing all extracted claim fields\n",
        "    missing_fields : list of str\n",
        "        List of mandatory fields that are missing\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of (str, str)\n",
        "        - route_name: Destination queue for the claim\n",
        "        - reasoning: Explanation for the routing decision\n",
        "\n",
        "    Routing Rules (in priority order)\n",
        "    ----------------------------------\n",
        "    1. Missing Fields → Manual Review\n",
        "       If any mandatory field is absent, route to human processing\n",
        "\n",
        "    2. Fraud Keywords → Investigation\n",
        "       If description contains suspicious words, flag for investigation\n",
        "       Keywords: \"fraud\", \"staged\", \"inconsistent\"\n",
        "\n",
        "    3. Injury Claims → Specialist Queue\n",
        "       If claim type is \"injury\", route to medical specialists\n",
        "\n",
        "    4. Low Damage → Fast Track\n",
        "       If estimated damage < $25,000, expedite processing\n",
        "\n",
        "    5. Default → Standard Processing\n",
        "       All other claims follow normal workflow\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Rules are evaluated sequentially\n",
        "    - First matching rule determines the route\n",
        "    - Fraud detection is case-insensitive\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data = {\"estimated_damage\": 15000, \"claim_type\": \"Property Damage\"}\n",
        "    >>> missing = []\n",
        "    >>> route, reason = route_claim(data, missing)\n",
        "    >>> print(route, reason)\n",
        "    Fast Track Low damage amount\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # RULE 1: MISSING MANDATORY FIELDS → MANUAL REVIEW\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Incomplete claims require human review to gather missing information\n",
        "\n",
        "    if missing_fields:\n",
        "        return (\n",
        "            \"Manual Review\",\n",
        "            f\"Missing mandatory fields: {', '.join(missing_fields)}\"\n",
        "        )\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # RULE 2: FRAUD KEYWORDS → INVESTIGATION\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Flag claims with suspicious language for fraud investigation\n",
        "\n",
        "    description = extracted_data.get(\"description\", \"\")\n",
        "\n",
        "    # List of suspicious keywords that trigger investigation\n",
        "    fraud_keywords = [\"fraud\", \"staged\", \"inconsistent\"]\n",
        "\n",
        "    if description:\n",
        "        description_lower = description.lower()\n",
        "\n",
        "        # Check if any fraud keyword appears in the description\n",
        "        detected_keywords = [\n",
        "            keyword for keyword in fraud_keywords\n",
        "            if keyword in description_lower\n",
        "        ]\n",
        "\n",
        "        if detected_keywords:\n",
        "            return (\n",
        "                \"Investigation\",\n",
        "                f\"Suspicious keywords detected: {', '.join(detected_keywords)}\"\n",
        "            )\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # RULE 3: INJURY CLAIMS → SPECIALIST QUEUE\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Route injury claims to medical specialists for proper assessment\n",
        "\n",
        "    claim_type = extracted_data.get(\"claim_type\", \"\")\n",
        "\n",
        "    if claim_type and claim_type.lower() == \"injury\":\n",
        "        return (\n",
        "            \"Specialist Queue\",\n",
        "            \"Injury claim requires medical specialist review\"\n",
        "        )\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # RULE 4: LOW DAMAGE → FAST TRACK\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Expedite small claims for faster resolution and customer satisfaction\n",
        "\n",
        "    estimated_damage = extracted_data.get(\"estimated_damage\", float('inf'))\n",
        "\n",
        "    # Fast-track threshold: $25,000\n",
        "    FAST_TRACK_THRESHOLD = 25000\n",
        "\n",
        "    if estimated_damage and estimated_damage < FAST_TRACK_THRESHOLD:\n",
        "        return (\n",
        "            \"Fast Track\",\n",
        "            f\"Low damage amount (${estimated_damage:,} < ${FAST_TRACK_THRESHOLD:,})\"\n",
        "        )\n",
        "\n",
        "    # -----------------------------------------------------------------------\n",
        "    # RULE 5: DEFAULT → STANDARD PROCESSING\n",
        "    # -----------------------------------------------------------------------\n",
        "    # Claims that don't match special criteria follow normal workflow\n",
        "\n",
        "    return (\n",
        "        \"Standard\",\n",
        "        \"No special conditions detected - standard processing workflow\"\n",
        "    )"
      ],
      "metadata": {
        "id": "jAeILLZw_Uw9"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 7: OUTPUT GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_output(extracted_data: Dict[str, Any],\n",
        "                   missing_fields: List[str],\n",
        "                   route: str,\n",
        "                   reasoning: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generate structured JSON output for the claim processing result.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    extracted_data : dict\n",
        "        All extracted fields from the claim document\n",
        "    missing_fields : list of str\n",
        "        List of mandatory fields that are missing\n",
        "    route : str\n",
        "        Recommended processing route\n",
        "    reasoning : str\n",
        "        Explanation for the routing decision\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Structured output in the required JSON format\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> output = generate_output(data, [], \"Fast Track\", \"Low damage\")\n",
        "    >>> print(json.dumps(output, indent=2))\n",
        "    \"\"\"\n",
        "\n",
        "    output = {\n",
        "        \"extractedFields\": extracted_data,\n",
        "        \"missingFields\": missing_fields,\n",
        "        \"recommendedRoute\": route,\n",
        "        \"reasoning\": reasoning\n",
        "    }\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "ie32s6jd_Yj5"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 8: MAIN EXECUTION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def process_fnol_document(pdf_file) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Complete pipeline to process an FNOL document.\n",
        "\n",
        "    This is the main orchestration function that coordinates all\n",
        "    processing steps from PDF reading to final JSON output.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pdf_file : file object\n",
        "        Binary file object of the PDF document\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Complete processing result with all extracted fields,\n",
        "        validation results, and routing recommendation\n",
        "\n",
        "    Pipeline Steps\n",
        "    --------------\n",
        "    1. Extract text from PDF\n",
        "    2. Parse structured fields\n",
        "    3. Identify missing mandatory fields\n",
        "    4. Apply routing rules\n",
        "    5. Generate structured output\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> with open(\"claim.pdf\", \"rb\") as f:\n",
        "    ...     result = process_fnol_document(f)\n",
        "    >>> print(result[\"recommendedRoute\"])\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"AUTONOMOUS INSURANCE CLAIMS PROCESSING AGENT\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Step 1: Extract text from PDF\n",
        "    print(\"\\n[Step 1/5] Extracting text from PDF...\")\n",
        "    text = extract_text_from_pdf(pdf_file)\n",
        "    print(f\"✓ Extracted {len(text)} characters\")\n",
        "\n",
        "    # Step 2: Parse structured fields\n",
        "    print(\"\\n[Step 2/5] Parsing claim fields...\")\n",
        "    extracted_data = extract_claim_fields(text)\n",
        "\n",
        "    # Count how many fields were successfully extracted\n",
        "    extracted_count = sum(1 for v in extracted_data.values() if v is not None)\n",
        "    print(f\"✓ Extracted {extracted_count}/{len(extracted_data)} fields\")\n",
        "\n",
        "    # Step 3: Validate mandatory fields\n",
        "    print(\"\\n[Step 3/5] Validating mandatory fields...\")\n",
        "    missing_fields = identify_missing_fields(extracted_data)\n",
        "\n",
        "    if missing_fields:\n",
        "        print(f\"⚠ Missing {len(missing_fields)} mandatory fields\")\n",
        "    else:\n",
        "        print(\"✓ All mandatory fields present\")\n",
        "\n",
        "    # Step 4: Apply routing rules\n",
        "    print(\"\\n[Step 4/5] Determining claim route...\")\n",
        "    route, reasoning = route_claim(extracted_data, missing_fields)\n",
        "    print(f\"✓ Route: {route}\")\n",
        "    print(f\"  Reason: {reasoning}\")\n",
        "\n",
        "    # Step 5: Generate output\n",
        "    print(\"\\n[Step 5/5] Generating output...\")\n",
        "    output = generate_output(extracted_data, missing_fields, route, reasoning)\n",
        "    print(\"✓ Output generated\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "3pLB3WiC_eYR"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 9: GOOGLE COLAB FILE UPLOAD INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "def upload_and_process():\n",
        "    \"\"\"\n",
        "    Interactive file upload and processing for Google Colab.\n",
        "\n",
        "    This function provides a browser-based file picker for users\n",
        "    to upload FNOL PDF documents and processes them automatically.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Processing result for the uploaded document\n",
        "    \"\"\"\n",
        "\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"Please upload your FNOL PDF document...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Get the first uploaded file\n",
        "    pdf_filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\nProcessing: {pdf_filename}\\n\")\n",
        "\n",
        "    # Process the document\n",
        "    with open(pdf_filename, \"rb\") as f:\n",
        "        result = process_fnol_document(f)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RESULTS (JSON Format)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(json.dumps(result, indent=2))\n",
        "\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "Q01t3Wsk_hig"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 10: COMMAND-LINE INTERFACE (FOR LOCAL EXECUTION)\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Main entry point for the script.\n",
        "\n",
        "    Usage:\n",
        "    ------\n",
        "    In Google Colab:\n",
        "        result = upload_and_process()\n",
        "\n",
        "    From command line:\n",
        "        python synapx_assignment_corrected.py claim.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    import sys\n",
        "\n",
        "    # Check if running in Google Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    # If in Colab, use upload interface\n",
        "    if IN_COLAB:\n",
        "        print(\"Running in Google Colab mode\")\n",
        "        print(\"Execute: upload_and_process()\")\n",
        "\n",
        "    # If command-line argument provided, process that file\n",
        "    elif len(sys.argv) > 1:\n",
        "        pdf_path = sys.argv[1]\n",
        "        print(f\"Processing file: {pdf_path}\")\n",
        "\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            result = process_fnol_document(f)\n",
        "\n",
        "        # Print result as formatted JSON\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"RESULTS (JSON Format)\")\n",
        "        print(\"=\" * 70)\n",
        "        print(json.dumps(result, indent=2))\n",
        "\n",
        "    else:\n",
        "        print(\"Usage:\")\n",
        "        print(\"  In Colab: upload_and_process()\")\n",
        "        print(\"  Command line: python synapx_assignment_corrected.py <pdf_file>\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebWOc9Y0_mdZ",
        "outputId": "abf55ce4-c488-4c38-cd04-018e071a9d41"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab mode\n",
            "Execute: upload_and_process()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upload_and_process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fhS7WsHj_qsI",
        "outputId": "7940bf99-b174-4c22-f026-2012aedabbe3"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your FNOL PDF document...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ab9d33e5-6d1b-4a61-99df-d4ae405d8f20\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ab9d33e5-6d1b-4a61-99df-d4ae405d8f20\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ACORD-Automobile-Loss-Notice-12.05.16.pdf to ACORD-Automobile-Loss-Notice-12.05.16 (3).pdf\n",
            "\n",
            "Processing: ACORD-Automobile-Loss-Notice-12.05.16 (3).pdf\n",
            "\n",
            "======================================================================\n",
            "AUTONOMOUS INSURANCE CLAIMS PROCESSING AGENT\n",
            "======================================================================\n",
            "\n",
            "[Step 1/5] Extracting text from PDF...\n",
            "✓ Extracted 13992 characters\n",
            "\n",
            "[Step 2/5] Parsing claim fields...\n",
            "✓ Extracted 0/18 fields\n",
            "\n",
            "[Step 3/5] Validating mandatory fields...\n",
            "⚠ Missing 10 mandatory fields\n",
            "\n",
            "[Step 4/5] Determining claim route...\n",
            "✓ Route: Manual Review\n",
            "  Reason: Missing mandatory fields: policy_number, policyholder_name, incident_date, location, description, claimant_name, asset_type, claim_type, estimated_damage, initial_estimate\n",
            "\n",
            "[Step 5/5] Generating output...\n",
            "✓ Output generated\n",
            "\n",
            "======================================================================\n",
            "PROCESSING COMPLETE\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "RESULTS (JSON Format)\n",
            "======================================================================\n",
            "{\n",
            "  \"extractedFields\": {\n",
            "    \"policy_number\": null,\n",
            "    \"policyholder_name\": null,\n",
            "    \"effective_date_start\": null,\n",
            "    \"effective_date_end\": null,\n",
            "    \"incident_date\": null,\n",
            "    \"incident_time\": null,\n",
            "    \"location\": null,\n",
            "    \"description\": null,\n",
            "    \"claimant_name\": null,\n",
            "    \"claimant_contact\": null,\n",
            "    \"third_party_name\": null,\n",
            "    \"third_party_contact\": null,\n",
            "    \"asset_type\": null,\n",
            "    \"asset_id\": null,\n",
            "    \"estimated_damage\": null,\n",
            "    \"claim_type\": null,\n",
            "    \"attachments\": null,\n",
            "    \"initial_estimate\": null\n",
            "  },\n",
            "  \"missingFields\": [\n",
            "    \"policy_number\",\n",
            "    \"policyholder_name\",\n",
            "    \"incident_date\",\n",
            "    \"location\",\n",
            "    \"description\",\n",
            "    \"claimant_name\",\n",
            "    \"asset_type\",\n",
            "    \"claim_type\",\n",
            "    \"estimated_damage\",\n",
            "    \"initial_estimate\"\n",
            "  ],\n",
            "  \"recommendedRoute\": \"Manual Review\",\n",
            "  \"reasoning\": \"Missing mandatory fields: policy_number, policyholder_name, incident_date, location, description, claimant_name, asset_type, claim_type, estimated_damage, initial_estimate\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extractedFields': {'policy_number': None,\n",
              "  'policyholder_name': None,\n",
              "  'effective_date_start': None,\n",
              "  'effective_date_end': None,\n",
              "  'incident_date': None,\n",
              "  'incident_time': None,\n",
              "  'location': None,\n",
              "  'description': None,\n",
              "  'claimant_name': None,\n",
              "  'claimant_contact': None,\n",
              "  'third_party_name': None,\n",
              "  'third_party_contact': None,\n",
              "  'asset_type': None,\n",
              "  'asset_id': None,\n",
              "  'estimated_damage': None,\n",
              "  'claim_type': None,\n",
              "  'attachments': None,\n",
              "  'initial_estimate': None},\n",
              " 'missingFields': ['policy_number',\n",
              "  'policyholder_name',\n",
              "  'incident_date',\n",
              "  'location',\n",
              "  'description',\n",
              "  'claimant_name',\n",
              "  'asset_type',\n",
              "  'claim_type',\n",
              "  'estimated_damage',\n",
              "  'initial_estimate'],\n",
              " 'recommendedRoute': 'Manual Review',\n",
              " 'reasoning': 'Missing mandatory fields: policy_number, policyholder_name, incident_date, location, description, claimant_name, asset_type, claim_type, estimated_damage, initial_estimate'}"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aMqyccYh_vn2"
      },
      "execution_count": 158,
      "outputs": []
    }
  ]
}